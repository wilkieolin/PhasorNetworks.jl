<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Networks · PhasorNetworks.jl</title><meta name="title" content="Networks · PhasorNetworks.jl"/><meta property="og:title" content="Networks · PhasorNetworks.jl"/><meta property="twitter:title" content="Networks · PhasorNetworks.jl"/><meta name="description" content="Documentation for PhasorNetworks.jl."/><meta property="og:description" content="Documentation for PhasorNetworks.jl."/><meta property="twitter:description" content="Documentation for PhasorNetworks.jl."/><meta property="og:url" content="https://wilkieolin.github.io/PhasorNetworks.jl/api/network/"/><meta property="twitter:url" content="https://wilkieolin.github.io/PhasorNetworks.jl/api/network/"/><link rel="canonical" href="https://wilkieolin.github.io/PhasorNetworks.jl/api/network/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">PhasorNetworks.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../types/">Types</a></li><li class="is-active"><a class="tocitem" href>Networks</a></li><li><a class="tocitem" href="../spiking/">Spiking</a></li><li><a class="tocitem" href="../domains/">Domains</a></li><li><a class="tocitem" href="../vsa/">VSA</a></li><li><a class="tocitem" href="../gpu/">GPU</a></li><li><a class="tocitem" href="../metrics/">Metrics</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Reference</a></li><li class="is-active"><a href>Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Networks</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/wilkieolin/PhasorNetworks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/main/docs/src/api/network.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Network-API"><a class="docs-heading-anchor" href="#Network-API">Network API</a><a id="Network-API-1"></a><a class="docs-heading-anchor-permalink" href="#Network-API" title="Permalink"></a></h1><p>Documentation for neural network implementations.</p><article><details class="docstring" open="true"><summary id="Core.Union-Tuple{SpikingCall, Union{AbstractArray, NamedTuple}, NamedTuple}"><a class="docstring-binding" href="#Core.Union-Tuple{SpikingCall, Union{AbstractArray, NamedTuple}, NamedTuple}"><code>Core.Union</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">(m::Union{MaxPool, MinPool})(x::SpikingCall, ps::LuxParams, st::NamedTuple)</code></pre><p>Extend MaxPool to handle SpikeTrain inputs by selecting the spike with maximum decoded phase value over the pooling dimensions.</p><p><strong>Arguments</strong></p><ul><li><code>x::SpikingCall</code>: Input spiking call</li><li><code>ps::LuxParams</code>: Layer parameters</li><li><code>st::NamedTuple</code>: Layer state</li></ul><p><strong>Operation</strong></p><ol><li>Converts each spike to its corresponding phase value using <code>train_to_phase</code></li><li>Finds the maximum phase value over the pooling dimensions</li><li>Returns a new SpikeTrain with the selected spike(s), preserving temporal offset</li></ol><p><strong>Returns</strong></p><ul><li><code>output_train::SpikeTrain</code>: Spike train containing only maximum phase spikes</li><li><code>st::NamedTuple</code>: Unchanged state</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L1340-L1359">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.Codebook"><a class="docstring-binding" href="#PhasorNetworks.Codebook"><code>PhasorNetworks.Codebook</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">Codebook &lt;: LuxCore.AbstractLuxLayer</code></pre><p>Layer that accesses a fixed set of phase codes and computes similarities with inputs. Used for discrete embedding or classification tasks in phase-based networks.</p><p><strong>Fields</strong></p><ul><li><code>dims::Pair{&lt;:Int, &lt;:Int}</code>: Input dimension =&gt; Number of codes</li></ul><p><strong>State</strong></p><ul><li><code>codes</code>: Random phase symbols initialized as the codebook</li><li>Codes are fixed after initialization (non-trainable)</li></ul><p><strong>Forward Pass</strong></p><ol><li>For phase inputs: Computes similarity with all codes</li><li>For spiking inputs: Converts codes to currents and computes temporal similarity</li></ol><p><strong>Use Cases</strong></p><ul><li>Discrete symbol encoding in Vector Symbolic Architectures</li><li>Classification by similarity to learned phase patterns</li><li>Phase-based memory or lookup mechanisms</li></ul><p>See also: <a href="../gpu/#PhasorNetworks.similarity_outer-Tuple{CUDA.CuArray{ComplexF32, 3}, CUDA.CuArray{ComplexF32, 3}}"><code>similarity_outer</code></a> for similarity computation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L603-L626">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.ComplexBias"><a class="docstring-binding" href="#PhasorNetworks.ComplexBias"><code>PhasorNetworks.ComplexBias</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ComplexBias &lt;: LuxCore.AbstractLuxLayer</code></pre><p>Layer that adds learnable complex-valued biases to phase networks.</p><p><strong>Fields</strong></p><ul><li><code>dims</code>: Dimensions of the bias terms</li><li><code>init_bias</code>: Function to initialize bias values (default: ones)</li></ul><p><strong>Initialization Options</strong></p><ul><li><code>default_bias</code>: Initialize with ones in complex plane</li><li><code>zero_bias</code>: Initialize with zeros</li><li>Custom initialization function with signature (rng, dims) -&gt; ComplexF32 array</li></ul><p>Used as a component in <a href="#PhasorNetworks.PhasorDense"><code>PhasorDense</code></a> and <a href="#PhasorNetworks.PhasorConv"><code>PhasorConv</code></a> layers to provide phase shifts in the complex plane.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L143-L159">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.MakeSpiking"><a class="docstring-binding" href="#PhasorNetworks.MakeSpiking"><code>PhasorNetworks.MakeSpiking</code></a> — <span class="docstring-category">Type</span></summary><section><div><p>MakeSpiking - a layer to include in Chains to convert phase tensors into SpikeTrains</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L1-L4">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.PhasorConv"><a class="docstring-binding" href="#PhasorNetworks.PhasorConv"><code>PhasorNetworks.PhasorConv</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">PhasorConv &lt;: LuxCore.AbstractLuxContainerLayer{(:layer, :bias)}</code></pre><p>A convolutional layer for phase-valued and spiking neural networks. Implements complex-valued convolution with phase-based activation, supporting both direct phase inputs and spike train inputs through ODE integration.</p><p><strong>Fields</strong></p><ul><li><code>layer</code>: Underlying <code>Conv</code> layer for spatial convolution (no internal bias)</li><li><code>bias</code>: <a href="#PhasorNetworks.ComplexBias"><code>ComplexBias</code></a> layer for phase shifts, shaped for broadcasting over spatial dims</li><li><code>activation</code>: Activation function applied after bias (e.g., <code>identity</code>, <code>complex_to_angle</code>)</li><li><code>use_bias::Bool</code>: Whether to apply the complex bias term</li><li><code>init_leakage::Function</code>: Initializer for per-channel leakage scaling factors</li><li><code>init_period::Function</code>: Initializer for per-channel period scaling factors</li><li><code>trainable_leakage::Bool</code>: If <code>true</code>, leakage factors are trainable parameters</li><li><code>trainable_period::Bool</code>: If <code>true</code>, period factors are trainable parameters</li><li><code>return_type::SolutionType</code>: Output format for spiking inputs</li></ul><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">PhasorConv(kernel::Tuple{Vararg{Integer}}, channels::Pair{&lt;:Integer,&lt;:Integer}, 
           activation=identity;
    return_type=SolutionType(:spiking),  # :phase, :potential, :current, or :spiking
    init_bias=default_bias,               # Bias initialization function
    use_bias=true,                         # Apply complex bias
    init_leakage=ones32,                   # Leakage factor initializer
    init_period=ones32,                    # Period factor initializer
    trainable_leakage=false,               # Make leakage trainable
    trainable_period=false,                # Make period trainable
    kwargs...)                             # Passed to Conv layer (stride, pad, dilation, etc.)</code></pre><p><strong>Arguments</strong></p><ul><li><code>kernel</code>: Tuple specifying convolution kernel size, e.g., <code>(3, 3)</code> for 2D</li><li><code>channels</code>: Input channels =&gt; Output channels, e.g., <code>3 =&gt; 16</code></li></ul><p><strong>Return Types (for spiking inputs)</strong></p><ul><li><code>:phase</code>: Extract phases from ODE solution, apply activation, return array</li><li><code>:potential</code>: Return raw ODE solution object</li><li><code>:current</code>: Convert solution to current, return <code>CurrentCall</code> for next layer</li><li><code>:spiking</code>: Convert solution to spike train, return <code>SpikingCall</code> (default)</li></ul><p><strong>Forward Pass</strong></p><p>For phase array inputs (H, W, C, B):</p><ol><li>Converts input phases to complex numbers via <code>angle_to_complex</code></li><li>Applies convolution separately to real and imaginary parts</li><li>Optionally adds complex bias (broadcast over spatial dimensions)</li><li>Applies activation function</li></ol><p>For spiking inputs (<code>SpikingCall</code> or <code>CurrentCall</code>):</p><ol><li>Converts to <code>CurrentCall</code> if needed</li><li>Integrates coupled oscillator ODEs via <code>oscillator_bank</code></li><li>Returns output based on <code>return_type</code></li></ol><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Basic conv layer: 3x3 kernel, 3 input channels, 16 output channels
layer = PhasorConv((3, 3), 3 =&gt; 16, complex_to_angle; pad=1)

# With trainable dynamics and strided convolution
layer = PhasorConv((5, 5), 16 =&gt; 32, complex_to_angle;
    stride=2,
    trainable_leakage=true,
    return_type=SolutionType(:current))

# Initialize and apply
rng = Random.default_rng()
ps, st = Lux.setup(rng, layer)
y, st_new = layer(x, ps, st)  # x: (H, W, C_in, batch)</code></pre><p>See also: <a href="#PhasorNetworks.PhasorDense"><code>PhasorDense</code></a>, <a href="#PhasorNetworks.PhasorFixed"><code>PhasorFixed</code></a>, <a href="#PhasorNetworks.ComplexBias"><code>ComplexBias</code></a>, <a href="../gpu/#PhasorNetworks.oscillator_bank-Tuple{SpikeTrainGPU{3}, CUDA.CuArray, CUDA.CuArray}"><code>oscillator_bank</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L404-L476">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.PhasorDense"><a class="docstring-binding" href="#PhasorNetworks.PhasorDense"><code>PhasorNetworks.PhasorDense</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">PhasorDense &lt;: LuxCore.AbstractLuxContainerLayer{(:layer, :bias)}</code></pre><p>A dense (fully-connected) layer for phase-valued and spiking neural networks. Implements complex-valued linear transformations with phase-based activation, supporting both direct phase inputs and spike train inputs through ODE integration.</p><p><strong>Fields</strong></p><ul><li><code>layer</code>: Underlying <code>Dense</code> layer for linear transformation (no internal bias)</li><li><code>bias</code>: <a href="#PhasorNetworks.ComplexBias"><code>ComplexBias</code></a> layer for phase shifts in the complex plane</li><li><code>activation</code>: Activation function applied after bias (e.g., <code>identity</code>, <code>complex_to_angle</code>)</li><li><code>use_bias::Bool</code>: Whether to apply the complex bias term</li><li><code>init_leakage::Function</code>: Initializer for per-neuron leakage scaling factors</li><li><code>init_period::Function</code>: Initializer for per-neuron period scaling factors</li><li><code>trainable_leakage::Bool</code>: If <code>true</code>, leakage factors are trainable parameters</li><li><code>trainable_period::Bool</code>: If <code>true</code>, period factors are trainable parameters</li><li><code>return_type::SolutionType</code>: Output format for spiking inputs</li></ul><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">PhasorDense(shape::Pair{&lt;:Integer,&lt;:Integer}, activation=identity;
    return_type=SolutionType(:spiking),  # :phase, :potential, :current, or :spiking
    init_bias=default_bias,               # Bias initialization function
    use_bias=true,                         # Apply complex bias
    init_leakage=ones32,                   # Leakage factor initializer
    init_period=ones32,                    # Period factor initializer
    trainable_leakage=false,               # Make leakage trainable
    trainable_period=false,                # Make period trainable
    kwargs...)                             # Passed to Dense layer</code></pre><p><strong>Return Types (for spiking inputs)</strong></p><ul><li><code>:phase</code>: Extract phases from ODE solution, apply activation, return array</li><li><code>:potential</code>: Return raw ODE solution object</li><li><code>:current</code>: Convert solution to current, return <code>CurrentCall</code> for next layer</li><li><code>:spiking</code>: Convert solution to spike train, return <code>SpikingCall</code> (default)</li></ul><p><strong>Forward Pass</strong></p><p>For phase array inputs:</p><ol><li>Converts input phases to complex numbers via <code>angle_to_complex</code></li><li>Applies linear transformation separately to real and imaginary parts</li><li>Optionally adds complex bias</li><li>Applies activation function</li></ol><p>For spiking inputs (<code>SpikingCall</code> or <code>CurrentCall</code>):</p><ol><li>Converts to <code>CurrentCall</code> if needed</li><li>Integrates coupled oscillator ODEs via <code>oscillator_bank</code></li><li>Returns output based on <code>return_type</code></li></ol><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Basic dense layer
layer = PhasorDense(64 =&gt; 32, complex_to_angle)

# With trainable dynamics
layer = PhasorDense(64 =&gt; 32, complex_to_angle;
    trainable_leakage=true,
    return_type=SolutionType(:current))

# Initialize and apply
rng = Random.default_rng()
ps, st = Lux.setup(rng, layer)
y, st_new = layer(x, ps, st)</code></pre><p>See also: <a href="#PhasorNetworks.PhasorConv"><code>PhasorConv</code></a>, <a href="#PhasorNetworks.PhasorFixed"><code>PhasorFixed</code></a>, <a href="#PhasorNetworks.ComplexBias"><code>ComplexBias</code></a>, <a href="../gpu/#PhasorNetworks.oscillator_bank-Tuple{SpikeTrainGPU{3}, CUDA.CuArray, CUDA.CuArray}"><code>oscillator_bank</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L209-L275">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.PhasorFixed"><a class="docstring-binding" href="#PhasorNetworks.PhasorFixed"><code>PhasorNetworks.PhasorFixed</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">PhasorFixed &lt;: LuxCore.AbstractLuxContainerLayer{(:layer, :bias)}</code></pre><p>A dense layer with non-trainable (fixed) weights for phase-valued and spiking networks. Weights are stored in layer state rather than parameters, making them immune to gradient updates while still supporting the full range of input types.</p><p><strong>Fields</strong></p><ul><li><code>layer</code>: Template <code>Dense</code> layer (weights stored in state, not used directly)</li><li><code>bias</code>: <a href="#PhasorNetworks.ComplexBias"><code>ComplexBias</code></a> layer for phase shifts (parameters also stored in state)</li><li><code>activation</code>: Activation function applied after bias</li><li><code>use_bias::Bool</code>: Whether to apply the complex bias term</li><li><code>init_leakage::Function</code>: Initializer for per-neuron leakage scaling factors</li><li><code>init_period::Function</code>: Initializer for per-neuron period scaling factors</li><li><code>trainable_leakage::Bool</code>: If <code>true</code>, leakage factors are trainable parameters</li><li><code>trainable_period::Bool</code>: If <code>true</code>, period factors are trainable parameters</li><li><code>return_type::SolutionType</code>: Output format for spiking inputs</li><li><code>init_weight</code>: Weight initialization function, or <code>nothing</code> for Glorot uniform</li></ul><p><strong>Constructor</strong></p><pre><code class="language-julia hljs">PhasorFixed(shape::Pair{&lt;:Integer,&lt;:Integer}, activation=identity;
    return_type=SolutionType(:spiking),  # :phase, :potential, :current, or :spiking
    init_weight=nothing,                   # Custom weight init: (rng, in, out) -&gt; matrix
    init_bias=default_bias,                # Bias initialization function
    use_bias=false,                        # Apply complex bias (default off)
    init_leakage=ones32,                   # Leakage factor initializer
    init_period=ones32,                    # Period factor initializer
    trainable_leakage=false,               # Make leakage trainable
    trainable_period=false,                # Make period trainable
    kwargs...)                             # Additional options</code></pre><p><strong>Weight Initialization Options</strong></p><ul><li><code>init_weight=nothing</code>: Uses Glorot uniform initialization (default)</li><li><code>init_weight=identity_init</code>: Creates identity matrix (requires square shape)</li><li>Custom function with signature <code>(rng, in_dim, out_dim) -&gt; weight_matrix</code></li></ul><p><strong>State Structure</strong></p><p>The layer state contains:</p><ul><li><code>weight</code>: The fixed weight matrix (out<em>dim × in</em>dim)</li><li><code>layer</code>: Nested structure with weight for <code>oscillator_bank</code> compatibility</li><li><code>bias_params</code>: Complex bias parameters (real and imaginary parts)</li><li><code>bias</code>: Bias layer state</li><li><code>leakage</code>: Per-neuron leakage factors (if not trainable)</li><li><code>period</code>: Per-neuron period factors (if not trainable)</li></ul><p><strong>Return Types (for spiking inputs)</strong></p><ul><li><code>:phase</code>: Extract phases from ODE solution, apply activation, return array</li><li><code>:potential</code>: Return raw ODE solution object</li><li><code>:current</code>: Convert solution to current, return <code>CurrentCall</code> for next layer</li><li><code>:spiking</code>: Convert solution to spike train, return <code>SpikingCall</code> (default)</li></ul><p><strong>Forward Pass</strong></p><p>For phase array inputs:</p><ol><li>Converts input phases to complex numbers via <code>angle_to_complex</code></li><li>Applies fixed weight matrix separately to real and imaginary parts</li><li>Optionally adds complex bias</li><li>Applies activation function</li></ol><p>For spiking inputs (<code>SpikingCall</code> or <code>CurrentCall</code>):</p><ol><li>Converts to <code>CurrentCall</code> if needed</li><li>Constructs pseudo-params from state for <code>oscillator_bank</code></li><li>Integrates coupled oscillator ODEs</li><li>Returns output based on <code>return_type</code></li></ol><p><strong>Example</strong></p><pre><code class="language-julia hljs"># Fixed random projection
layer = PhasorFixed(64 =&gt; 64, complex_to_angle)

# Identity layer (no transformation)
identity_init(rng, in_dim, out_dim) = Matrix{Float32}(I, out_dim, in_dim)
layer = PhasorFixed(64 =&gt; 64, identity; init_weight=identity_init)

# Fixed layer with trainable dynamics
layer = PhasorFixed(128 =&gt; 64, complex_to_angle;
    trainable_leakage=true,
    return_type=SolutionType(:current))

# Initialize and apply
rng = Random.default_rng()
ps, st = Lux.setup(rng, layer)
y, st_new = layer(x, ps, st)

# Weights are in state, not params
@assert !haskey(ps, :weight)  # No weights in params
@assert haskey(st, :weight)   # Weights in state</code></pre><p><strong>Use Cases</strong></p><ul><li>Fixed random projections for reservoir computing</li><li>Identity or permutation mappings</li><li>Frozen pretrained layers in transfer learning</li><li>Non-trainable feature extractors</li><li>Dimensionality-preserving transformations</li></ul><p>See also: <a href="#PhasorNetworks.PhasorDense"><code>PhasorDense</code></a> for trainable version, <a href="#PhasorNetworks.ComplexBias"><code>ComplexBias</code></a>, <a href="../gpu/#PhasorNetworks.oscillator_bank-Tuple{SpikeTrainGPU{3}, CUDA.CuArray, CUDA.CuArray}"><code>oscillator_bank</code></a></p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L662-L760">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.ResidualBlock"><a class="docstring-binding" href="#PhasorNetworks.ResidualBlock"><code>PhasorNetworks.ResidualBlock</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ResidualBlock &lt;: LuxCore.AbstractLuxContainerLayer{(:ff,)}</code></pre><p>Residual block for phase-based neural networks, implementing skip connections through phase binding.</p><p><strong>Fields</strong></p><ul><li><code>ff</code>: Feed-forward chain of phase-based layers</li></ul><p><strong>Implementation Details</strong></p><ol><li>Processes input through feed-forward path</li><li>Binds (combines) original input with processed output</li><li>Maintains phase-based representation throughout</li></ol><p>Used to build deep phase networks while mitigating phase degradation, similar to residual connections in standard neural networks but using phase binding for combination.</p><p>See also: <a href="../vsa/#PhasorNetworks.v_bind-Tuple{AbstractArray}"><code>v_bind</code></a> for the phase binding operation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L990-L1009">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.SingleHeadAttention"><a class="docstring-binding" href="#PhasorNetworks.SingleHeadAttention"><code>PhasorNetworks.SingleHeadAttention</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">SingleHeadAttention &lt;: LuxCore.AbstractLuxContainerLayer{(:q_proj, :k_proj, :v_proj, :attention, :out_proj)}</code></pre><p>Single-head attention mechanism for phase-based transformers. Implements attention using phase similarity for key-query interactions.</p><p><strong>Fields</strong></p><ul><li><code>q_proj</code>: Query projection layer</li><li><code>k_proj</code>: Key projection layer</li><li><code>v_proj</code>: Value projection layer</li><li><code>attention</code>: Attention scoring mechanism</li><li><code>out_proj</code>: Output projection layer</li></ul><p><strong>Implementation Details</strong></p><ol><li>Projects input to query/key/value representations</li><li>Computes attention scores using phase similarity</li><li>Combines values weighted by attention scores</li><li>Projects combined values to output space</li></ol><p>Can operate on both direct phase inputs and spiking representations. See also: <a href="#PhasorNetworks.attend-Tuple{Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}}"><code>attend</code></a> for the core attention computation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L1136-L1157">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.TrackOutput"><a class="docstring-binding" href="#PhasorNetworks.TrackOutput"><code>PhasorNetworks.TrackOutput</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">TrackOutput{L&lt;:Lux.AbstractLuxLayer} &lt;: Lux.AbstractLuxLayer</code></pre><p>Wrapper layer that records intermediate outputs during forward passes. Useful for analyzing internal representations in phase networks.</p><p><strong>Fields</strong></p><ul><li><code>layer::L</code>: The layer whose outputs to track</li></ul><p><strong>State</strong></p><p>Maintains a tuple of all intermediate outputs in the state.outputs field. Each forward pass appends its output to this tuple.</p><p><strong>Usage</strong></p><pre><code class="language-julia hljs">tracked_layer = TrackOutput(PhasorDense(64 =&gt; 32))
y, st = tracked_layer(x, ps, st)
intermediate_outputs = st.outputs  # Access all recorded outputs</code></pre><p>Useful for visualization, analysis, and debugging of phase networks.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L1395-L1416">source</a></section></details></article><article><details class="docstring" open="true"><summary id="LuxLib.API.dropout-Union{Tuple{T}, Tuple{Random.AbstractRNG, SpikingCall, T, Any, T, Any}} where T"><a class="docstring-binding" href="#LuxLib.API.dropout-Union{Tuple{T}, Tuple{Random.AbstractRNG, SpikingCall, T, Any, T, Any}} where T"><code>LuxLib.API.dropout</code></a> — <span class="docstring-category">Method</span></summary><section><div><p>Extension of dropout to SpikeTrains</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L126-L128">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.attend-Tuple{Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}}"><a class="docstring-binding" href="#PhasorNetworks.attend-Tuple{Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}}"><code>PhasorNetworks.attend</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">attend(q::SpikingTypes, k::SpikingTypes, v::SpikingTypes; spk_args, tspan, return_solution=false, scale=[1.0f0]) -&gt; Tuple</code></pre><p>Compute attention between spiking neural representations using phase similarity. Core attention mechanism for spiking transformer architectures.</p><p><strong>Arguments</strong></p><ul><li><code>q, k, v</code>: Query, key, and value spike trains</li><li><code>spk_args::SpikingArgs</code>: Spiking neuron parameters</li><li><code>tspan</code>: Time span for simulation</li><li><code>return_solution::Bool</code>: Whether to return raw potentials</li><li><code>scale::AbstractArray</code>: Attention scaling factor</li></ul><p><strong>Implementation</strong></p><ol><li>Computes temporal similarity between query and key spikes</li><li>Converts value spikes to oscillator potentials</li><li>Scales and combines values based on similarities</li><li>Optionally converts back to spike train</li></ol><p>Returns:</p><ul><li>Spike train or potentials representing attended values</li><li>Attention scores over time</li></ul><p>See also: <a href="#PhasorNetworks.attend-Tuple{Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}}"><code>attend</code></a> for phase-based version</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L1070-L1094">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.train-NTuple{6, Any}"><a class="docstring-binding" href="#PhasorNetworks.train-NTuple{6, Any}"><code>PhasorNetworks.train</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">train(model, ps, st, train_loader, loss, args; optimiser=Optimisers.Adam, verbose=false, sample_gradients=0)</code></pre><p>Train a phase-based neural network using gradient descent.</p><p><strong>Arguments</strong></p><ul><li><code>model</code>: Network model (any Lux.jl compatible architecture)</li><li><code>ps</code>: Model parameters</li><li><code>st</code>: Model state</li><li><code>train_loader</code>: Data loader providing (x, y) batches</li><li><code>loss</code>: Loss function(x, y, model, params, state)</li><li><code>args::Args</code>: Training configuration</li><li><code>optimiser</code>: Optimization algorithm (default: Adam)</li><li><code>verbose::Bool</code>: Whether to print loss values</li><li><code>sample_gradients::Int</code>: Frequency of gradient sampling (0 to disable)</li></ul><p><strong>Returns</strong></p><ul><li><code>losses</code>: Array of loss values during training</li><li><code>ps</code>: Updated parameters</li><li><code>st</code>: Updated state</li><li><code>gradients</code>: Sampled gradients if enabled</li></ul><p>Automatically handles CPU/GPU device placement based on args.use_cuda.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L1236-L1259">source</a></section></details></article><article><details class="docstring" open="true"><summary id="PhasorNetworks.variance_scaling-Tuple{Random.AbstractRNG, Vararg{Integer}}"><a class="docstring-binding" href="#PhasorNetworks.variance_scaling-Tuple{Random.AbstractRNG, Vararg{Integer}}"><code>PhasorNetworks.variance_scaling</code></a> — <span class="docstring-category">Method</span></summary><section><div><pre><code class="language-julia hljs">variance_scaling(rng::AbstractRNG, shape::Integer...; mode=&quot;avg&quot;, scale=0.66f0)</code></pre><p>Initialize network weights using variance scaling initialization. Adapts the scale based on input/output dimensions to maintain stable variances.</p><p><strong>Arguments</strong></p><ul><li><code>rng::AbstractRNG</code>: Random number generator</li><li><code>shape::Integer...</code>: Dimensions of weight matrix/tensor</li><li><code>mode::String</code>: Scaling mode (&quot;fan<em>in&quot;, &quot;fan</em>out&quot;, or &quot;avg&quot;)</li><li><code>scale::Real</code>: Base scaling factor (default: 0.66f0)</li></ul><p><strong>Modes</strong></p><ul><li>&quot;fan_in&quot;: Scale based on input dimension</li><li>&quot;fan_out&quot;: Scale based on output dimension</li><li>&quot;avg&quot;: Scale based on average of input/output dimensions</li></ul><p>Returns weights initialized from truncated normal distribution with computed standard deviation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/a4b760fc75839323144814a60eef779d84b45471/src/network.jl#L1437-L1456">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../types/">« Types</a><a class="docs-footer-nextpage" href="../spiking/">Spiking »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Monday 16 February 2026 20:33">Monday 16 February 2026</span>. Using Julia version 1.11.9.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
