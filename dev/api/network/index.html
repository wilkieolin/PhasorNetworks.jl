<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Networks · PhasorNetworks.jl</title><meta name="title" content="Networks · PhasorNetworks.jl"/><meta property="og:title" content="Networks · PhasorNetworks.jl"/><meta property="twitter:title" content="Networks · PhasorNetworks.jl"/><meta name="description" content="Documentation for PhasorNetworks.jl."/><meta property="og:description" content="Documentation for PhasorNetworks.jl."/><meta property="twitter:description" content="Documentation for PhasorNetworks.jl."/><meta property="og:url" content="https://wilkieolin.github.io/PhasorNetworks.jl/api/network/"/><meta property="twitter:url" content="https://wilkieolin.github.io/PhasorNetworks.jl/api/network/"/><link rel="canonical" href="https://wilkieolin.github.io/PhasorNetworks.jl/api/network/"/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../../">PhasorNetworks.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><span class="tocitem">API Reference</span><ul><li><a class="tocitem" href="../types/">Types</a></li><li class="is-active"><a class="tocitem" href>Networks</a></li><li><a class="tocitem" href="../spiking/">Spiking</a></li><li><a class="tocitem" href="../domains/">Domains</a></li><li><a class="tocitem" href="../vsa/">VSA</a></li><li><a class="tocitem" href="../gpu/">GPU</a></li><li><a class="tocitem" href="../metrics/">Metrics</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API Reference</a></li><li class="is-active"><a href>Networks</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Networks</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/wilkieolin/PhasorNetworks.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/main/docs/src/api/network.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Network-API"><a class="docs-heading-anchor" href="#Network-API">Network API</a><a id="Network-API-1"></a><a class="docs-heading-anchor-permalink" href="#Network-API" title="Permalink"></a></h1><p>Documentation for neural network implementations.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.Codebook" href="#PhasorNetworks.Codebook"><code>PhasorNetworks.Codebook</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">Codebook &lt;: LuxCore.AbstractLuxLayer</code></pre><p>Layer that accesses a fixed set of phase codes and computes similarities with inputs. Used for discrete embedding or classification tasks in phase-based networks.</p><p><strong>Fields</strong></p><ul><li><code>dims::Pair{&lt;:Int, &lt;:Int}</code>: Input dimension =&gt; Number of codes</li></ul><p><strong>State</strong></p><ul><li><code>codes</code>: Random phase symbols initialized as the codebook</li><li>Codes are fixed after initialization (non-trainable)</li></ul><p><strong>Forward Pass</strong></p><ol><li>For phase inputs: Computes similarity with all codes</li><li>For spiking inputs: Converts codes to currents and computes temporal similarity</li></ol><p><strong>Use Cases</strong></p><ul><li>Discrete symbol encoding in Vector Symbolic Architectures</li><li>Classification by similarity to learned phase patterns</li><li>Phase-based memory or lookup mechanisms</li></ul><p>See also: <a href="../gpu/#PhasorNetworks.similarity_outer-Tuple{CUDA.CuArray{ComplexF32, 3}, CUDA.CuArray{ComplexF32, 3}}"><code>similarity_outer</code></a> for similarity computation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L385-L408">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.ComplexBias" href="#PhasorNetworks.ComplexBias"><code>PhasorNetworks.ComplexBias</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ComplexBias &lt;: LuxCore.AbstractLuxLayer</code></pre><p>Layer that adds learnable complex-valued biases to phase networks.</p><p><strong>Fields</strong></p><ul><li><code>dims</code>: Dimensions of the bias terms</li><li><code>init_bias</code>: Function to initialize bias values (default: ones)</li></ul><p><strong>Initialization Options</strong></p><ul><li><code>default_bias</code>: Initialize with ones in complex plane</li><li><code>zero_bias</code>: Initialize with zeros</li><li>Custom initialization function with signature (rng, dims) -&gt; ComplexF32 array</li></ul><p>Used as a component in <a href="#PhasorNetworks.PhasorDense"><code>PhasorDense</code></a> and <a href="#PhasorNetworks.PhasorConv"><code>PhasorConv</code></a> layers to provide phase shifts in the complex plane.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L145-L161">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.MakeSpiking" href="#PhasorNetworks.MakeSpiking"><code>PhasorNetworks.MakeSpiking</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>MakeSpiking - a layer to include in Chains to convert phase tensors into SpikeTrains</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L3-L6">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.PhasorConv" href="#PhasorNetworks.PhasorConv"><code>PhasorNetworks.PhasorConv</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PhasorConv &lt;: LuxCore.AbstractLuxContainerLayer{(:layer, :bias)}</code></pre><p>Convolutional layer for phase-valued inputs and spiking neural networks. Implements complex-valued convolution with phase-based activation.</p><p><strong>Fields</strong></p><ul><li><code>layer</code>: Standard convolutional layer for spatial operations</li><li><code>bias</code>: Complex-valued bias terms</li><li><code>activation</code>: Phase activation function</li><li><code>use_bias::Bool</code>: Whether to apply complex bias</li><li><code>return_solution::Bool</code>: Whether to return full ODE solution for spiking inputs</li></ul><p><strong>Implementation</strong></p><ul><li>Separates input into real/imaginary components</li><li>Applies convolution separately to each component</li><li>Recombines into complex values</li><li>Optionally applies complex bias and activation</li></ul><p>See also: <a href="#PhasorNetworks.PhasorDense"><code>PhasorDense</code></a> for fully-connected equivalent</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L297-L317">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.PhasorDense" href="#PhasorNetworks.PhasorDense"><code>PhasorNetworks.PhasorDense</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PhasorDense &lt;: LuxCore.AbstractLuxContainerLayer{(:layer, :bias)}</code></pre><p>A dense (fully-connected) layer that operates on phase/complex-valued inputs. Combines a standard dense layer with complex bias and phase-based activation.</p><p><strong>Fields</strong></p><ul><li><code>layer</code>: Standard dense layer for linear transformation</li><li><code>bias</code>: Complex-valued bias for phase shift</li><li><code>activation</code>: Function to convert complex values to phases</li><li><code>use_bias::Bool</code>: Whether to apply complex bias</li><li><code>return_solution::Bool</code>: Whether to return full ODE solution for spiking inputs</li></ul><p><strong>Layer Operation</strong></p><ol><li>Converts input phases to complex numbers</li><li>Applies linear transformation separately to real/imaginary parts</li><li>Optionally applies complex bias</li><li>Applies activation function to map back to phases</li></ol><p>Supports both direct phase inputs and spiking inputs through ODEs.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L211-L231">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.PhasorFixed" href="#PhasorNetworks.PhasorFixed"><code>PhasorNetworks.PhasorFixed</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">PhasorFixed &lt;: Lux.AbstractLuxLayer</code></pre><p>A layer that responds to input currents or spikes with a predetermined phase pattern. The weights are fixed (e.g., identity matrix) and non-trainable.</p><p><strong>Fields</strong></p><ul><li><code>shape::Int</code>: Layer dimension</li><li><code>layer</code>: The underlying fixed phase pattern layer</li><li><code>init_weight</code>: Function to initialize the fixed weight pattern</li><li><code>return_solution::Bool</code>: Whether to return the phase solution</li><li><code>static::Bool</code>: If true, maintains fixed phase relationships</li></ul><p><strong>Forward Pass</strong></p><ol><li>Receives input currents or spikes</li><li>Neurons respond according to fixed weight pattern</li><li>Produces output phases and spikes based on predetermined pattern</li></ol><p><strong>Use Cases</strong></p><ul><li>Fixed pattern detection</li><li>Identity or permutation mapping</li><li>Simple phase transformations with predetermined patterns</li></ul><p>See also: <a href="../types/#PhasorNetworks.SpikeTrain"><code>SpikeTrain</code></a> for spike input/output format</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L442-L466">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.ResidualBlock" href="#PhasorNetworks.ResidualBlock"><code>PhasorNetworks.ResidualBlock</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">ResidualBlock &lt;: LuxCore.AbstractLuxContainerLayer{(:ff,)}</code></pre><p>Residual block for phase-based neural networks, implementing skip connections through phase binding.</p><p><strong>Fields</strong></p><ul><li><code>ff</code>: Feed-forward chain of phase-based layers</li></ul><p><strong>Implementation Details</strong></p><ol><li>Processes input through feed-forward path</li><li>Binds (combines) original input with processed output</li><li>Maintains phase-based representation throughout</li></ol><p>Used to build deep phase networks while mitigating phase degradation, similar to residual connections in standard neural networks but using phase binding for combination.</p><p>See also: <a href="../vsa/#PhasorNetworks.v_bind-Tuple{AbstractArray}"><code>v_bind</code></a> for the phase binding operation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L586-L605">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.SingleHeadAttention" href="#PhasorNetworks.SingleHeadAttention"><code>PhasorNetworks.SingleHeadAttention</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">SingleHeadAttention &lt;: LuxCore.AbstractLuxContainerLayer{(:q_proj, :k_proj, :v_proj, :attention, :out_proj)}</code></pre><p>Single-head attention mechanism for phase-based transformers. Implements attention using phase similarity for key-query interactions.</p><p><strong>Fields</strong></p><ul><li><code>q_proj</code>: Query projection layer</li><li><code>k_proj</code>: Key projection layer</li><li><code>v_proj</code>: Value projection layer</li><li><code>attention</code>: Attention scoring mechanism</li><li><code>out_proj</code>: Output projection layer</li></ul><p><strong>Implementation Details</strong></p><ol><li>Projects input to query/key/value representations</li><li>Computes attention scores using phase similarity</li><li>Combines values weighted by attention scores</li><li>Projects combined values to output space</li></ol><p>Can operate on both direct phase inputs and spiking representations. See also: <a href="#PhasorNetworks.attend-Tuple{Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}}"><code>attend</code></a> for the core attention computation</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L718-L739">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.TrackOutput" href="#PhasorNetworks.TrackOutput"><code>PhasorNetworks.TrackOutput</code></a> — <span class="docstring-category">Type</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">TrackOutput{L&lt;:Lux.AbstractLuxLayer} &lt;: Lux.AbstractLuxLayer</code></pre><p>Wrapper layer that records intermediate outputs during forward passes. Useful for analyzing internal representations in phase networks.</p><p><strong>Fields</strong></p><ul><li><code>layer::L</code>: The layer whose outputs to track</li></ul><p><strong>State</strong></p><p>Maintains a tuple of all intermediate outputs in the state.outputs field. Each forward pass appends its output to this tuple.</p><p><strong>Usage</strong></p><pre><code class="language-julia hljs">tracked_layer = TrackOutput(PhasorDense(64 =&gt; 32))
y, st = tracked_layer(x, ps, st)
intermediate_outputs = st.outputs  # Access all recorded outputs</code></pre><p>Useful for visualization, analysis, and debugging of phase networks.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L903-L924">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="LuxLib.API.dropout-Union{Tuple{T}, Tuple{Random.AbstractRNG, SpikingCall, T, Any, T, Any}} where T" href="#LuxLib.API.dropout-Union{Tuple{T}, Tuple{Random.AbstractRNG, SpikingCall, T, Any, T, Any}} where T"><code>LuxLib.API.dropout</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><p>Extension of dropout to SpikeTrains</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L128-L130">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.attend-Tuple{Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}}" href="#PhasorNetworks.attend-Tuple{Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}}"><code>PhasorNetworks.attend</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">attend(q::SpikingTypes, k::SpikingTypes, v::SpikingTypes; spk_args, tspan, return_solution=false, scale=[1.0f0]) -&gt; Tuple</code></pre><p>Compute attention between spiking neural representations using phase similarity. Core attention mechanism for spiking transformer architectures.</p><p><strong>Arguments</strong></p><ul><li><code>q, k, v</code>: Query, key, and value spike trains</li><li><code>spk_args::SpikingArgs</code>: Spiking neuron parameters</li><li><code>tspan</code>: Time span for simulation</li><li><code>return_solution::Bool</code>: Whether to return raw potentials</li><li><code>scale::AbstractArray</code>: Attention scaling factor</li></ul><p><strong>Implementation</strong></p><ol><li>Computes temporal similarity between query and key spikes</li><li>Converts value spikes to oscillator potentials</li><li>Scales and combines values based on similarities</li><li>Optionally converts back to spike train</li></ol><p>Returns:</p><ul><li>Spike train or potentials representing attended values</li><li>Attention scores over time</li></ul><p>See also: <a href="#PhasorNetworks.attend-Tuple{Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}, Union{SpikeTrain, SpikeTrainGPU}}"><code>attend</code></a> for phase-based version</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L656-L680">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.train-NTuple{6, Any}" href="#PhasorNetworks.train-NTuple{6, Any}"><code>PhasorNetworks.train</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">train(model, ps, st, train_loader, loss, args; optimiser=Optimisers.Adam, verbose=false, sample_gradients=0)</code></pre><p>Train a phase-based neural network using gradient descent.</p><p><strong>Arguments</strong></p><ul><li><code>model</code>: Network model (any Lux.jl compatible architecture)</li><li><code>ps</code>: Model parameters</li><li><code>st</code>: Model state</li><li><code>train_loader</code>: Data loader providing (x, y) batches</li><li><code>loss</code>: Loss function(x, y, model, params, state)</li><li><code>args::Args</code>: Training configuration</li><li><code>optimiser</code>: Optimization algorithm (default: Adam)</li><li><code>verbose::Bool</code>: Whether to print loss values</li><li><code>sample_gradients::Int</code>: Frequency of gradient sampling (0 to disable)</li></ul><p><strong>Returns</strong></p><ul><li><code>losses</code>: Array of loss values during training</li><li><code>ps</code>: Updated parameters</li><li><code>st</code>: Updated state</li><li><code>gradients</code>: Sampled gradients if enabled</li></ul><p>Automatically handles CPU/GPU device placement based on args.use_cuda.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L815-L838">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="PhasorNetworks.variance_scaling-Tuple{Random.AbstractRNG, Vararg{Integer}}" href="#PhasorNetworks.variance_scaling-Tuple{Random.AbstractRNG, Vararg{Integer}}"><code>PhasorNetworks.variance_scaling</code></a> — <span class="docstring-category">Method</span><span class="is-flex-grow-1 docstring-article-toggle-button" title="Collapse docstring"></span></header><section><div><pre><code class="language-julia hljs">variance_scaling(rng::AbstractRNG, shape::Integer...; mode=&quot;avg&quot;, scale=0.66f0)</code></pre><p>Initialize network weights using variance scaling initialization. Adapts the scale based on input/output dimensions to maintain stable variances.</p><p><strong>Arguments</strong></p><ul><li><code>rng::AbstractRNG</code>: Random number generator</li><li><code>shape::Integer...</code>: Dimensions of weight matrix/tensor</li><li><code>mode::String</code>: Scaling mode (&quot;fan<em>in&quot;, &quot;fan</em>out&quot;, or &quot;avg&quot;)</li><li><code>scale::Real</code>: Base scaling factor (default: 0.66f0)</li></ul><p><strong>Modes</strong></p><ul><li>&quot;fan_in&quot;: Scale based on input dimension</li><li>&quot;fan_out&quot;: Scale based on output dimension</li><li>&quot;avg&quot;: Scale based on average of input/output dimensions</li></ul><p>Returns weights initialized from truncated normal distribution with computed standard deviation.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/wilkieolin/PhasorNetworks.jl/blob/0f41c61d7b04b24eac6517cd1ebe12308e7830e2/src/network.jl#L945-L964">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../types/">« Types</a><a class="docs-footer-nextpage" href="../spiking/">Spiking »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.15.0 on <span class="colophon-date" title="Wednesday 12 November 2025 19:51">Wednesday 12 November 2025</span>. Using Julia version 1.11.7.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
